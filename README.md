# Gemini Terminal Agent

A developer-focused terminal agent powered by Google's Gemini AI API. This project provides a flexible command-line interface for AI code generation, file management, and project scaffolding.

## Technical Overview

This agent uses the Google Generative AI Python SDK to interact with the Gemini 1.5 model for content generation. Key technical components:

- **Python 3.9+** - Core runtime environment
- **Google Generative AI SDK** - Interfaces with Gemini models
- **Regular Expressions** - Parses code blocks and extracts file paths
- **Command Pattern** - Extensible command processing system
- **File I/O** - Handles code generation with proper directory structures

## Setup for Development

### Prerequisites
- Python 3.9+
- Google API key with Gemini access
- Development environment with terminal access

### Installation

```bash
# Clone the repository
git clone https://github.com/Yukrant/coding_agent.git
cd coding_agent

# Create virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Set up environment
cp .env.example .env
# Edit .env and add your API key:
# GOOGLE_API_KEY=your_api_key_here
# GEMINI_MODEL_NAME=gemini-1.5-flash  # Optional: Change to your preferred model
```

## Architecture

The codebase is organized into several modules, each with distinct responsibilities:

- `agent.py` - Main entry point and command processor
- `llm.py` - Gemini API integration
- `executor.py` - Shell command execution
- `utils.py` - File handling utilities

### Core Components

#### Command System
The agent uses an alias-based command system that maps user input to specific functions. Commands are processed through the `process_command()` function, which handles command parsing and execution.

#### File Processing
Code blocks generated by the AI are automatically parsed and saved with appropriate file extensions. The system can detect code languages and organize files in logical project structures.

#### Project Templates
Project scaffolding is implemented through the `initialize_project()` function, which creates directory structures and base files for different project types.

## Extending the Agent

### Adding New Commands

To add a new command:

1. Update the `COMMAND_ALIASES` dictionary with your command name and aliases
2. Add a new condition in the `process_command()` function
3. Implement the function that handles your command's logic

Example:

```python
# Add to COMMAND_ALIASES
COMMAND_ALIASES = {
    # ... existing aliases
    "mynewcmd": "!mynewcmd",
    "mnc": "!mynewcmd",
}

# In process_command()
elif command.startswith("!mynewcmd "):
    param = command[10:].strip()
    return my_new_command_function(param)

# Implement function
def my_new_command_function(param):
    # Your command logic here
    return f"Command executed with: {param}"
```

### Adding Project Templates

To add a new project template:

1. Update the `initialize_project()` function with a new condition for your project type
2. Define the directory structure and file contents for your template

Example:

```python
elif project_type.lower() == "express":
    # Create Express.js structure
    dirs = [
        "routes",
        "controllers",
        "models",
        "public",
        "views"
    ]
    files = {
        "app.js": "// Express app code here",
        "package.json": "{ \"name\": \"express-app\" }"
    }
    # ... create directories and files
```

## API Integration

The agent uses the Google Generative AI Python SDK to communicate with Gemini. The default model is configured in the `.env` file:

```
GOOGLE_API_KEY=your_api_key_here
GEMINI_MODEL_NAME=gemini-1.5-flash
```

You can modify these settings to use different Gemini models or configure generation parameters in `llm.py`:

```python
def chat_with_gemini(prompt):
    model = genai.GenerativeModel(model_name)  # Uses model from .env
    
    # Configure model parameters
    generation_config = {
        "temperature": 0.7,
        "top_p": 0.95,
        "top_k": 40,
        "max_output_tokens": 8192,
    }
    
    response = model.generate_content(
        prompt,
        generation_config=generation_config
    )
    
    return response.text
```

## Security Considerations

The agent implements several security measures:

- Path validation to prevent directory traversal
- Command filtering to block potentially dangerous operations
- Input validation for all user-provided data
